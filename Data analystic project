PANDAS
Welcome to the lesson on Pandas.Pandas is a package for data manipulation and analysis in Python. The name Pandas is derived from the econometrics term Panel Data. Pandas incorporates two additional data structures into Python, namely Pandas Series and Pandas DataFrame. These data structures allow us to work with labeled and relational data in an easy and intuitive manner. These lessons are intended as a basic overview of Pandas and introduces some of its most important features.
Why Use Pandas?
The recent success of machine learning algorithms is partly due to the huge amounts of data that we have available to train our algorithms on. However, when it comes to data, quantity is not the only thing that matters, the quality of your data is just as important.

More often than not, large datasets will often have missing values, outliers, incorrect values, etcâ€¦ Having data with a lot of missing or bad values, for example, is not going to allow your machine learning algorithms to perform well.

his is where Pandas come in. Pandas Series and DataFrames are designed for fast data analysis and manipulation, as well as being flexible and easy to use. Below are just a few features that makes Pandas an excellent package for data analysis:

Allows the use of labels for rows and columns
Can calculate rolling statistics on time series data
Easy handling of NaN values
Is able to load data of different formats into DataFrames
Can join and merge different datasets together
It integrates with NumPy and Matplotlib
For these and other reasons, Pandas DataFrames have become one of the most commonly used Pandas object for data analysis in Python.

PANDAS SERIES:
A Pandas series is a one-dimensional array-like object that can hold many data types, such as numbers or strings. One of the main differences between Pandas Series and NumPy ndarrays is that you can assign an index label to each element in the Pandas Series. In other words, you can name the indices of your Pandas Series anything you want. Another big difference between Pandas Series and NumPy ndarrays is that Pandas Series can hold data of different data types.

`# This is formatted as code`
[ ]
# We first have to import pandas using import statement in python

import pandas as pd
Let's create a pandas series. For that we just write pd.Series()

items = pd.Series(data, index)    # For now we just names the series object in a variable named item
So while creating series we can pass arguments for data and index

[ ]
item = pd.Series(data=[15, 5, 'No'], index=['chocolates', 'chips', 'milk'])
print(item)
chocolates    15
chips          5
milk          No
dtype: object
As we see the series is displayed as indicies in the first column ans data in the second column.




Attributes of Panda Series:
Let's see some of the attributes of the pandas series that helps us to understand our series

[ ]
item.shape   # gives us the  sizes of each dimension of the data
(3,)
[ ]
item.ndim   # gives us the number of dimensions of the data
1
[ ]
item.size   # gives us the total number of items in the array 
3
[ ]
item.index   # gives us the list indeices of the series 
Index(['chocolates', 'chips', 'milk'], dtype='object')
[ ]
item.values    #gives us the data of the series
array([15, 5, 'No'], dtype=object)
If you are dealing with a very large Pandas Series and if you are not sure whether an index label exists, you can check by using the in command

Accessing Elements in Pandas Series:
Now let's look at how we can access or modify elements in a Pandas Series. One great advantage of Pandas Series is that it allows us to access data in many different ways. Elements can be accessed using index labels or numerical indices inside square brackets, [ ], similar to how we access elements in NumPy ndarrays. Since we can use numerical indices, we can use both positive and negative integers to access data from the beginning or from the end of the Series, respectively.

[ ]
# One by their index labels
item['chocolates']
15
[ ]
item[['chocolates', 'milk']]   #a list of indices is passed 
chocolates    15
milk          No
dtype: object

Arithmatic operations on Pandas Series:
Just like with NumPy ndarrays, we can perform element-wise arithmetic operations on Pandas Series. We will look at the arithematic operations between pandas series and single numbers.
Let's first make a new series

[ ]
sweets = pd.Series(data=[10, 5, 7], index=['candies', 'donuts', 'ladoos'])
sweets
candies    10
donuts      5
ladoos      7
dtype: int64
We can now modify the data in sweets by performing basic arithmetic operations. Let's see some examples

[ ]
sweets+2
candies    12
donuts      7
ladoos      9
dtype: int64
[ ]
sweets-2
candies    8
donuts     3
ladoos     5
dtype: int64
[ ]
sweets*2
candies    20
donuts     10
ladoos     14
dtype: int64
[ ]
sweets/2
candies    5.0
donuts     2.5
ladoos     3.5
dtype: float64
[ ]
# WE CAN ALSO APPLY MATHEMATICAL FUNCTIONS FROM NUPY SUCH AS SQAURE ROOT
import numpy as np

np.sqrt(sweets)
candies    3.162278
donuts     2.236068
ladoos     2.645751
dtype: float64
Pandas also allows us to only apply arithmetic operations on selected items in our sweets list. Let's see some examples

[ ]
np.power(sweets, 4)
candies    10000
donuts       625
ladoos      2401
dtype: int64
[ ]
np.exp(sweets)
candies    22026.465795
donuts       148.413159
ladoos      1096.633158
dtype: float64
We can also apply arithemeatic operations on specific elements




PANDAS DATAFRAME
Dataframe is a two dimensional object which holds rows and columns and can hold values of different data types.

We can create a dataframe manually or by loading data from a file.

Let's first ceate a dataframe manually:<
First let's create a dictionary of pandas series and pass it into pandas dataframe
[ ]
item = {'Shaurya': pd.Series([250, 15, 70, 100], index=['watch', 'toys', 'glasses', 'shirt']),
        'Ashish': pd.Series([120, 50, 90], index=['pants', 'books', 'toys' ]), 
        'ShapeAI': pd.Series([120, 50, 90], index=['shirt', 'books', 'toys' ])}

# item is a dictionary for two people containing some items and the cost of the item 
[ ]
# WE CAN CREATE A DATA FRAME BY PASSINF THE DICTIONARY TO THE DataFrame FUNCTION

cart = pd.DataFrame(item)
cart

Make sure to capitalize the D and F while calling the dataframe function.
The dataframe is displayed in the tabular form
The row labels for the dataframe are built from the union of the index labels we provided in the series and the column labels for the dataframe is taken from the keys of the dictionaries.
The dataframe has NaN values because for Shaurya we have no item like books and pants in the dicrionary we provided and similarly we have NaN values for column Ashish.
[ ]
# IN ABOVE EXAMPLE WE PROVIDED THE DICTIONARIES THAT CLEARLY DEFINED THE INDEX LABLES, HOWEVER IF WE DON'T PROVIDE THE INDEX LABELS, 
# THEN THE DATAFRAME WOULD USE THE NUMERICAL INDEX VALUES
# LET'S CREATE THE SAME DICIONARY WITHOUT THE INDEXED LABELS
new_item = {'Shaurya': pd.Series([250, 15, 70, 100]),
        'Ashish': pd.Series([120, 50, 90])}


# NOW MAKE THE DATAFRAME USING THE NEW DICTIONARIES 
df = pd.DataFrame(new_item)
df

The dataframe uses the numerical indices.


2.a Attributes
Like we did in pandas series, we can also extract information from pandas dataframe using some attributes.

[ ]
cart.index
Index(['books', 'glasses', 'pants', 'shirt', 'toys', 'watch'], dtype='object')
[ ]
cart.columns
Index(['Shaurya', 'Ashish', 'ShapeAI'], dtype='object')
[ ]
cart.values
array([[ nan,  50.],
       [ 70.,  nan],
       [ nan, 120.],
       [100.,  nan],
       [ 15.,  90.],
       [250.,  nan]])
[ ]
cart.shape
(6, 3)
[ ]
cart.ndim
2
[ ]
cart.size
18
NOTE: While creating the cart dataframe, we passed the whole dictionary to the dataframe function. However, there might be cases when we are only interested in some specific subset of the whole data. Pandas let's us select which data we want to put into the DataFrame, with the keywords columns and index.

[ ]
ashish_cart = pd.DataFrame(item, columns=['Ashish'])
ashish_cart

[ ]
selected_item = pd.DataFrame(item, index=['pants', 'toys'])
selected_item

[ ]
ashish_selected_item = pd.DataFrame(item, columns=['Ashish','Shaurya'], index=['pants', 'toys'])
ashish_selected_item

We can also create a dataframe from a dictionary of lists or arrays. The procedure is same as before, we start by creating the dictionary and then pass it into the dataframe function. In this case however all the list or arrays in the dictionary must be of the same length.

[ ]
# Here's the dictionary of the integers and the floats.
data = {'Integers':[1,2,3],
         'Floats':[1.1, 2.2, 3.3]}

df = pd.DataFrame(data, index=['label1', 'label2', 'label3'])    ## IF WE DON'T PASS THE INDICES, DATAFRAME WILL AUTOMATICALLY USE NUMERICAL INDICES
df

[ ]
# Creating DataFrame using a list of python dictionaries 
ListOfDict = [{'apple':20, 'banana':15, 'orange':30},{'apple':10, 'tomato':17, 'grapes': 35}]

df = pd.DataFrame(ListOfDict)
df

DataFrame used the numerical row indices. If we want to assign the row indices some values, we can use:

[ ]
df.index = ['personA', 'personB']
df  

Accessing the values in a DataFrame

[ ]
df[['apple']]     ##accessing a column

[ ]
df[['banana','tomato']]     ##accessing by passing a list of columns

[ ]
df.loc[['personA','personB']]    ##accessing a row

[ ]
df.iloc[[1,0]]    ##accessing a rowusing iloc -> integer location

[ ]
df['grapes']['personB']    ##accessing a specific value
35.0
NOTE: While accessing the specific element, the column label always comes the first then the row label.

[ ]
## IF WE WANT TO ADD A NEW COLUMN TO OUR DATAFRAME,  WE CAN ADD LIKE THIS:

df['corn'] = [5, 7]
df

We can also add new columns using the arithematic operations on the other columns of our DataFrame.
For eg: we can add a new column vegies by adding the values for corn and tomato

[ ]
new_person = [{'apple':15, 'banana': 17, 'corn': 3, 'orange':5}]
new_df = pd.DataFrame(new_person, index=['personC'])
new_df

[ ]
## WE CAN NOW ADD THE NEW ROW TO THE ORIGINAL DATAFRAME

df = df.append(new_df)
df

Dealing with NaN values:
As mentioned earlier, before we can begin training our learning algorithms with large datasets, we usually need to clean the data first. This means we need to have a method for detecting and correcting errors in our data.

While any given dataset can have many types of bad data, such as outliers or incorrect values, the type of bad data we encounter almost always is missing values. As we saw earlier, Pandas assigns NaN values to missing data. In this lesson we will learn how to detect and deal with NaN values.

We will begin by creating a DataFrame with some NaN values in it.

[ ]
import pandas as pd

# We create a list of Python dictionaries
items2 = [{'bikes': 20, 'pants': 30, 'watches': 35, 'shirts': 15, 'shoes':8, 'suits':45},
{'watches': 10, 'glasses': 50, 'bikes': 15, 'pants':5, 'shirts': 2, 'shoes':5, 'suits':7},
{'bikes': 20, 'pants': 30, 'watches': 35, 'glasses': 4, 'shoes':10}]

# We create a DataFrame  and provide the row index
store_items = pd.DataFrame(items2, index = ['store 1', 'store 2', 'store 3'])

# We display the DataFrame
store_items

In cases where we load very large datasets into a DataFrame, possibly with millions of items, the number of NaN values is not easily visualized. For these cases, we can use a combination of methods to count the number of NaN values in our data. The following example combines the .isnull() and the sum() methods to count the number of NaN values in our DataFrame

[ ]
store_items.isnull()

In Pandas, logical True values have numerical value 1 and logical False values have numerical value 0. Therefore, we can count the number of NaN values by counting the number of logical True values.

[ ]
# We count the number of NaN values in the columns of store_items
x =  store_items.isnull().sum()

# We print x
print('Number of NaN values in our DataFrame:', x)
Number of NaN values in our DataFrame: bikes      0
pants      0
watches    0
shirts     1
shoes      0
suits      1
glasses    1
dtype: int64
Instead of counting the number of NaN values we can also do the opposite, we can count the number of non-NaN values. We can do this by using the .count() method as shown below:

[ ]
# We print the number of non-NaN values in our DataFrame
print()
print('Number of non-NaN values in the columns of our DataFrame:\n', 
        store_items.count())

Number of non-NaN values in the columns of our DataFrame:
 bikes      3
pants      3
watches    3
shirts     2
shoes      3
suits      2
glasses    2
dtype: int64
Now that we learned how to know if our dataset has any NaN values in it, the next step is to decide what to do with them. In general we have two options, we can either delete or replace the NaN values. In the following examples we will show you how to do both.

We will start by learning how to eliminate rows or columns from our DataFrame that contain any NaN values. The .dropna(axis) method eliminates any rows with NaN values when axis = 0 is used and will eliminate any columns with NaN values when axis = 1 is used. Let's see some examples

[ ]
# We drop any rows with NaN values
store_items.dropna(axis = 0)

[ ]
# We drop any columns with NaN values
store_items.dropna(axis = 1)

Notice that the .dropna() method eliminates (drops) the rows or columns with NaN values out of place. This means that the original DataFrame is not modified. You can always remove the desired rows or columns in place by setting the keyword inplace = True inside the dropna() function.

Now, instead of eliminating NaN values, we can replace them with suitable values. We could choose for example to replace all NaN values with the value 0. We can do this by using the .fillna() method as shown below.

[ ]
# We replace all NaN values with 0
store_items.fillna(0)

In machine learning you will most likely use databases from many sources to train your learning algorithms. Pandas allows us to load databases of different formats into DataFrames. One of the most popular data formats used to store databases is csv. CSV stands for Comma Separated Values and offers a simple format to store data. We can load CSV files into Pandas DataFrames using the pd.read_csv() function. Let's load Google stock data into a Pandas DataFrame. The GOOG.csv file contains Google stock data from 8/19/2004 till 10/13/2017 taken from Yahoo Finance.

[ ]
import pandas as pd

# We load Google stock data in a DataFrame
Google_stock = pd.read_csv('/content/sample_data/stocks.csv')

# We print some information about Google_stock
print('Google_stock is of type:', type(Google_stock))
print('Google_stock has shape:', Google_stock.shape)
Google_stock is of type: <class 'pandas.core.frame.DataFrame'>
Google_stock has shape: (3313, 7)
We see that we have loaded the stocks.csv file into a Pandas DataFrame and it consists of 3,313 rows and 7 columns. Now let's look at the stock data

[ ]
Google_stock

We see that it is quite a large dataset and that Pandas has automatically assigned numerical row indices to the DataFrame. Pandas also used the labels that appear in the data in the CSV file to assign the column labels.

When dealing with large datasets like this one, it is often useful just to take a look at the first few rows of data instead of the whole dataset. We can take a look at the first 5 rows of data using the .head() method, as shown below

[ ]
Google_stock.head(10)

We can also take a look at the last 5 rows of data by using the .tail() method:

[ ]
Google_stock.tail(7)

We can also optionally use .head(N) or .tail(N) to display the first and last N rows of data, respectively.

Let's do a quick check to see whether we have any NaN values in our dataset. To do this, we will use the .isnull() method followed by the .any() method to check whether any of the columns contain NaN values.

[ ]
Google_stock.isnull().any()
Date         False
Open         False
High         False
Low          False
Close        False
Adj Close    False
Volume       False
dtype: bool
Why Learn Data Visualization?
If you do any work with data, whether that be through business, machine learning, or artificial intelligence, you're going to want to know about how to put together visualizations. Visualizations serve two major purposes in the data analysis process.

First of all, visualizations can be used during your initial data exploration to help you make insights into your data. Visualizations can reveal patterns in the data that statistics alone might not provide. They can also provide an intuition about the structure of your data. In addition to answering your research questions, visualizations might promote new questions for further investigation.

Secondly, visualizations can help you convey your insights to others through explanatory plots. By generating new visualizations or polishing and refining the plots created during exploration, you can more easily show others what you found in your analysis. A good visualization can communicate your findings quickly and succinctly.

[ ]
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb

%matplotlib inline
[ ]
pokemon = pd.read_csv('/content/pokemon.csv')
print(pokemon.shape)
pokemon.head()

Bar Charts:
A bar chart is used to depict the distribution of a categorical variable. In a bar chart, each level of the categorical variable is depicted with a bar, whose height indicates the frequency of data points that take on that level. A basic bar chart of frequencies can be created through the use of seaborn's countplot function:

[ ]
sb.countplot(data = pokemon, x = 'generation_id');

By default, each category is given a different color. This might come in handy for building associations between these category labels and encodings in plots with more variables. Otherwise, it's a good idea to simplify the plot and reduce unnecessary distractions by plotting all bars in the same color. This can be set using the "color" parameter:

Additional Variations
If you have a lot of category levels, or the category names are long, then you might end up with overcrowding of the tick labels.

[ ]
base_color = sb.color_palette()[2]
sb.countplot(data = pokemon, x = 'type_1', color = 'cyan')

one way to deal with it is that, you can use matplotlib's xticks function and its "rotation" parameter to change the orientation in which the labels will be depicted (as degrees counter-clockwise from horizontal):
[ ]
base_color = sb.color_palette()[2]
sb.countplot(data = pokemon, x = 'type_1', color = base_color)
plt.xticks(rotation=90);

Another way to address this is through creation of a horizontal bar chart. In a horizontal bar chart, it is the length of each bar that indicates frequency, rather than the height. In the code, instead of setting the data or variable on the "x" parameter, you can set the variable to be plotted on the parameter "y":
[ ]
base_color = sb.color_palette()[2]
sb.countplot(data = pokemon, y = 'type_1', color = base_color)

Count Missing Data:
One interesting way we can apply bar charts is through the visualization of missing data. We can use pandas functions to create a table with the number of missing values in each column.

[ ]
pokemon.isna().sum()
id                   0
species              0
generation_id        0
height               0
weight               0
base_experience      0
type_1               0
type_2             402
hp                   0
attack               0
defense              0
speed                0
special-attack       0
special-defense      0
dtype: int64
What if we want to visualize these missing value counts? We could treat the variable names as levels of a categorical variable, and create a resulting bar plot. However, since the data is not in its tidy, unsummarized form, we need to make use of a different plotting function. Seaborn's barplot function is built to depict a summary of one quantitative variable against levels of a second, qualitative variable, but can be used here.

[ ]
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb

%matplotlib inline
[ ]
df = pd.read_csv('/content/pokemon.csv')
print(df.shape)
df.head()

Pie Charts:
A pie chart is a common univariate plot type that is used to depict relative frequencies for levels of a categorical variable. Frequencies in a pie chart are depicted as wedges drawn on a circle: the larger the angle or area, the more common the categorical value taken.

l3-c07-piecharts2.png

Pie chart and barchart displaying the same categorical counts.
Unfortunately, pie charts are a fairly limited plot type in the range of scenarios where they can be used, and it is easy for chart makers to try and spice up pie charts in a way that makes them more difficult to read. If you want to use a pie chart, try to follow certain guidelines:

Make sure that your interest is in relative frequencies. Areas should represent parts of a whole, rather than measurements on a second variable (unless that second variable can logically be summed up into some whole).
Limit the number of slices plotted. A pie chart works best with two or three slices, though it's also possible to plot with four or five slices as long as the wedge sizes can be distinguished. If you have a lot of categories, or categories that have small proportional representation, consider grouping them together so that fewer wedges are plotted, or use an 'Other' category to handle them.
Plot the data systematically. One typical method of plotting a pie chart is to start from the top of the circle, then plot each categorical level clockwise from most frequent to least frequent. If you have three categories and are interested in the comparison of two of them, a common plotting method is to place the two categories of interest on either side of the 12 o'clock direction, with the third category filling in the remaining space at the bottom.
If these guidelines cannot be met, then you should probably make use of a bar chart instead. A bar chart is a safer choice in general. The bar heights are more precisely interpreted than areas or angles, and a bar chart can be displayed more compactly than a pie chart. There's also more flexibility with a bar chart for plotting variables with a lot of levels, like plotting the bars horizontally.

You can create a pie chart with matplotlib's pie function. This function requires that the data be in a summarized form: the primary argument to the function will be the wedge sizes.

[ ]
# code for the pie chart seen above
sorted_counts = df['type_1'].value_counts()
plt.pie(sorted_counts, labels = sorted_counts.index);


Additional Variation:
A sister plot to the pie chart is the donut plot. It's just like a pie chart, except that there's a hole in the center of the plot. Perceptually, there's not much difference between a donut plot and a pie chart, and donut plots should be used with the same guidelines as a pie chart. Aesthetics might be one of the reasons why you would choose one or the other. For instance, you might see statistics reported in the hole of a donut plot to better make use of available space.

To create a donut plot, you can add a "wedgeprops" argument to the pie function call. By default, the radius of the pie (circle) is 1; setting the wedges' width property to less than 1 removes coloring from the center of the circle.

[ ]
sorted_counts = df['generation_id'].value_counts()
plt.pie(sorted_counts, labels = sorted_counts.index, wedgeprops = {'width' : 0.4});
plt.axis('square');

Histograms:
A histogram is used to plot the distribution of a numeric variable. It's the quantitative version of the bar chart. However, rather than plot one bar for each unique numeric value, values are grouped into continuous bins, and one bar for each bin is plotted depicting the number. For instance, using the default settings for matplotlib's hist function:

[ ]
plt.hist(data = df, x = 'speed')

Overall, a generally bimodal distribution is observed (one with two peaks or humps). The direct adjacency of the bars in the histogram, in contrast to the separated bars in a bar chart, emphasize the fact that the data takes on a continuous range of values. When a data value is on a bin edge, it is counted in the bin to its right. The exception is the rightmost bin edge, which places data values equal to the uppermost limit into the right-most bin (to the upper limit's left).

By default, the hist function divides the data into 10 bins, based on the range of values taken. In almost every case, we will want to change these settings. Usually, having only ten bins is too few to really understand the distribution of the data.

[ ]
plt.hist(data = df, x = 'speed', bins = 20)

You can use descriptive statistics (e.g. via df['num_var'].describe()) to gauge what minimum and maximum bin limits might be appropriate for the plot. These bin edges can be set using numpy's arange function.

The first argument to arange is the leftmost bin edge, the second argument the upper limit, and the third argument the bin width. Note that even though I've specified the "max" value in the second argument, I've added a "+1" (the bin width). That is because arange will only return values that are strictly less than the upper limit. Adding in "+1" is a safety measure to ensure that the rightmost bin edge is at least the maximum data value, so that all of the data points are plotted. The leftmost bin is set as a hardcoded value to get a nice, interpretable value, though you could use functions like numpy's around if you wanted to approach that end programmatically.

